{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "xSDP4bMrIL2x"
   },
   "source": [
    "# Decision Tree\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.   **[Introduction to Decision Trees](#1.-Introduction-to-Decision-Trees)**\n",
    "2.   **[Foundations of Decision Trees](#2.-Foundations-of-Decision-Trees)**\n",
    "3.   **[Decision Tree Tuning](#3.-Decision-Tree-Tuning)**\n",
    "4.   **[Model Validation](#4.-Model-Validation)**\n",
    "5.   **[Exploratory Data Analysis](#5.-Exploratory-Data-Analysis)**\n",
    "6.   **[Model Construction](#6.-Model-Construction)**\n",
    "7.   **[Model Results & Evaluation](#7.-Model-Results-&-Evaluation)**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"1.-Introduction-to-Decision-Trees\"></a>\n",
    "### 1. Introduction to Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decision Tree |** Flow-chart-like supervised classification model, and a representation of various solutions that are available to solve a given problem, based on teh possible outcomes of related choices\n",
    "- Require no assumptions regarding distribution of data\n",
    "- Handles collinearity very easily\n",
    "- Often doesn't require data preprocessing\n",
    "- Can be used for both classification and continuous variable prediction\n",
    "\n",
    "**Root Node |** The first node of the tree, where the first decision is made\n",
    "\n",
    "**Decision Node |** Nodes of the tree where decisions are made\n",
    "\n",
    "**Child Node |** A node that is pointed to from another node\n",
    "\n",
    "**Leaf Node |** The nodes where a final prediction is made"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"2.-Foundations-Decision-Trees\"></a>\n",
    "### 2. Foundations of Decision Trees"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Structure of a Decision Tree\n",
    "Decision trees are made of nodes. Nodes are groups of samples. There are different types of nodes, depending on how they function in the tree. The first node in a decision tree is called the **root node**. The first split always comes off of the root node, which divides the samples into two new nodes based on the values they contain for a particular feature.\n",
    "\n",
    "These two new nodes are referred to as **child nodes** of the root. A child node is any node that results from a split. The node that the child splits from is known as the **parent node.** Each of these two new child nodes in turn splits the data again, based on a new criterion. This process continues until the nodes stop splitting. The bottom-level nodes that do not split are called **leaf nodes**. All the nodes above the leaf nodes are called **decision nodes**, because they all make a decision that sorts the data either to the left or to the right.\n",
    "\n",
    "#### 2.2 Decisions and Splits\n",
    "In a decision tree, the data is split and passed down through decision nodes until reaching a leaf node. A decision node is split on the criterion that minimizes the impurity of the classes in their resulting children.\n",
    "\n",
    "##### 2.2.1 Categorical variables\n",
    "If the predictor variable is categorical, the decision tree algorithm will consider splitting based on category.\n",
    "\n",
    "##### 2.2.2 Continuous variables\n",
    "If the predictor variable is continuous, splits can be made anywhere along the range of numbers that exist in the data.\n",
    "\n",
    "#### 2.3 Choosing splits: Gini Impurity\n",
    "There are several possible metrics to use to determine the purity of a node and to decide how to split, including Gini impurity, entropy, information gain, and log loss. The most straightforward is Gini impurity.\n",
    "\n",
    "The Gini impurity of a node is defined as: $\\text{Gini Impurity} = 1 - \\sum\\limits_{i=1}^{n}P(i)^2$\n",
    "\n",
    "- where $i$= class\n",
    "- $P(i)$ = the probability of samples belonging to class $i$ in a given node\n",
    "\n",
    "The Gini impurity is calculated for each child node of each potential split point.\n",
    "\n",
    "#### 2.4 Advantages and Disadvantages of Decision Trees\n",
    "\n",
    "**Advantages:**\n",
    "\n",
    "- Require relatively few pre-processing steps\n",
    "- Can work easily with all types of variables (continuous, categorical, discrete)\n",
    "- Do not require normalization or scaling\n",
    "- Decisions are transparent\n",
    "- Not affected by extreme univariate values\n",
    "\n",
    "**Disadvantages:**\n",
    "\n",
    "- Can be computationally expensive relative to other algorithms\n",
    "- Small changes in data can result in significant changes in predictions\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"3.-Decision-Tree-Tuning\"></a>\n",
    "### 3. Decision Tree Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree tuning refers to the process of adjusting the hyperparameters to manipulate the structure of a decision tree algorithm with the aim of improving its performance on a given task.\n",
    "\n",
    "**Common tuning definitions:**\n",
    "- Max Depth: Defines how \"long\" a decision tree can get\n",
    "- Min samples leaf: defines the minimum number of samples for a leaf node\n",
    "- GridSearch: A tool to confirm that a model achieves its intended purpose by systematically checking every combination of hyperparameters to identify which set produces the best results, based on the selected metric"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"4.-Model-Validation\"></a>\n",
    "### 4. Model Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model validation is the set of processes and activities intended to verify that models are performing as expected. Model validation refers to the whole process of evaluating different models, selecting one, and then continuing to analyze the performance of the selected model to better understand its strengths and limitations. \n",
    "\n",
    "#### 4.1 Validation Sets \n",
    "The simplest way to maintain the objectivity of the test data is to create another partition in the data—a validation set—and save the test data for after you select the final model. The validation set is then used, instead of the test set, to compare different models.\n",
    "\n",
    "When building a model using a separate validation set, once the final model is selected, best practice is to go back and fit the selected model to all the non-test data (i.e., the training data + validation data) before scoring this final model on the test data.\n",
    "\n",
    "\n",
    "#### 4.2 Cross Validation \n",
    "Cross Validation is a process that uses different portions of the data to test and train a model on different iterations.  \n",
    "\n",
    "Cross-validation makes more efficient use of the training data by splitting the training data into k number of “folds” (partitions), training a model on k – 1 folds, and using the fold that was held out to get a validation score. The training process occurs k times, each time using a different fold as the validation set. At the end, the final validation score is the average of all k scores. This process is also commonly referred to as k-fold cross validation.\n",
    "\n",
    "After a model is selected using cross-validation, that selected model is then refit to the entire training set (i.e., it’s retrained on all k folds combined).\n",
    "\n",
    "Cross-validation reduces the likelihood of significant divergence of the distributions in the validation data from those in the full dataset. For this reason, it’s often the preferred technique when working with smaller datasets, which are more susceptible to randomness. The more folds you use, the more thorough the validation. However, adding folds increases the time needed to train, and may not be useful beyond a certain point.\n",
    "\n",
    "#### 4.3 Model Selection\n",
    "Once candidate models have been trained and validated a champion model is selected. Model validation scores factor heavily into this selection but score is seldom the only criterion. Often other factors are also considered. \n",
    "- How explainable is the model? \n",
    "- How complex is it? \n",
    "- How resilient is it against fluctuations in input values? \n",
    "- How well does it perform on data not found in the training data? \n",
    "- How much computational cost does it have to make predictions? \n",
    "- Does it add much latency to any production system? \n",
    "It’s not uncommon for a model with a slightly lower validation score to be selected over the highest-scoring model due to it being simpler, less computationally expensive, or more stable.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"5.-Exploratory-Data-Analysis\"></a>\n",
    "### 5. Exploratory Data Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard operational package imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Important imports for modeling and evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import plot_tree\n",
    "import sklearn.metrics as metrics\n",
    "\n",
    "# Visualization package imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset into a DataFrame and save in a variable\n",
    "data = pd.read_csv(\"example_file.csv\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Data Exploration\n",
    "After loading the dataset, the next step is to prepare the data to be suitable for clustering. This includes: \n",
    "\n",
    "*   Exploring data\n",
    "*   Checking for missing values\n",
    "*   Encoding categorical data \n",
    "*   Dropping irrelevant columns\n",
    "*   Renaming columns\n",
    "*   Create training and testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the first 10 rows of the data\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display number of rows, number of columns\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the data type for each column. NB logistic regression models expect numeric data\n",
    "data.dtypes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1 Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing values.\n",
    "# Save DataFrame in variable `data_subset`.\n",
    "data_subset = data.dropna(axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values.\n",
    "data_subset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first 10 rows.\n",
    "data_subset.head(10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.2 Drop Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the island column.\n",
    "data_subset = data_subset.drop(['irrelevant_column'], axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.3 Encode Data\n",
    "\n",
    "Decision trees require numeric columns so all relevant columns (target and predictor variables) must be converted accordingly through the process of One-hot encoding or Label encoding\n",
    "- Columns requiring special attention should be dealt with separately (special focus on target variable)\n",
    "- Finally all columns can be converted to numeric using `pandas.get_dummies()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Represent the data in the target variable numerically\n",
    "data_subset['target_variable'] = data_subset['target_variable'].map({\"class_1\": 1, \"class_2\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert all categorical columns to numeric.\n",
    "data_subset = pd.get_dummies(data_subset, drop_first = True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.4 Create Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put 75% of the data into a training set and the remaining 25% into a testing set.\n",
    "y = data_subset[\"target_variable\"]\n",
    "\n",
    "X = data_subset.copy()\n",
    "X = X.drop(\"target_variable\", axis = 1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"6.-Model-Construction\"></a>\n",
    "### 6. Model Construction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate decision tree and pass random_state= 0 so that the results can be replicated  \n",
    "decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Fit the model on the training set\n",
    "decision_tree.fit(X_train, y_train)\n",
    "\n",
    "# Use predict() to test the model and assign results to dt_pred\n",
    "dt_pred = decision_tree.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a name=\"7.-Model-Results-&-Evaluation\"></a>\n",
    "### 7. Model Results & Evaluation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1 Model Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the decision tree model's accuracy, precision, recall, and F1 score\n",
    "print(\"Decision Tree\")\n",
    "print(\"Accuracy:\", \"%.6f\" % metrics.accuracy_score(y_test, dt_pred))\n",
    "print(\"Precision:\", \"%.6f\" % metrics.precision_score(y_test, dt_pred))\n",
    "print(\"Recall:\", \"%.6f\" % metrics.recall_score(y_test, dt_pred))\n",
    "print(\"F1 Score:\", \"%.6f\" % metrics.f1_score(y_test, dt_pred))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Are there any additional steps that could improve the performance or function of this decision tree?\n",
    "\n",
    "Decision trees can be particularly susceptible to overfitting. Combining hyperparameter tuning and grid search can help ensure this doesn't happen. For instance, setting an appropriate value for max depth could potentially help reduce a decision tree's overfitting problem by limiting how deep a tree can grow."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2 Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model type errors by plotting a confusion matrix\n",
    "cm = metrics.confusion_matrix(y_test, dt_pred, labels = decision_tree.classes_)\n",
    "disp = metrics.ConfusionMatrixDisplay(confusion_matrix = cm,display_labels = decision_tree.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What patterns can be identified between true positives and true negatives, as well as false positives and false negatives?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3 Model Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot_tree function to produce a visual representation of the tree to pinpoint where the splits in the data are occurring\n",
    "plt.figure(figsize=(20,12))\n",
    "plot_tree(decision_tree, max_depth=2, fontsize=14, feature_names=X.columns); # max_depth set to 2 as the first splits are indicating the features with highest prediction value"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.4 Evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncover which features might be most important by building a feature importance graph\n",
    "importances = model.best_estimator_.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = forest_importances.sort_values(ascending=True)\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot the sorted feature importances with swapped x and y axis\n",
    "sorted_importances.plot.barh(ax=ax)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Relative Feature Importance - Random Forest')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Based on the feature importance graph, which features are the most important for this model?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.5 Hyperparameter Tuning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the best values for the hyperparameters `max_depth` and `min_samples_leaf` using grid search and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the hyperparameter values that are to be evaluated (the more values the longer the runtime so be strategic)\n",
    "tree_para = {'max_depth':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,30,40,50],\n",
    "             'min_samples_leaf': [2,3,4,5,6,7,8,9,10, 15, 20, 50]}\n",
    "\n",
    "# Create the scoring system\n",
    "scoring = {'accuracy', 'precision', 'recall', 'f1'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check every combination of values to examine which pair has the best evaluation metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a decision tree instance called `tuned_decision_tree` with `random_state=0`\n",
    "tuned_decision_tree = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "# Instantiate a `GridSearchCV` instance called `clf`\n",
    "clf = GridSearchCV(tuned_decision_tree, \n",
    "                   tree_para, \n",
    "                   scoring = scoring, \n",
    "                   cv=5, \n",
    "                   refit=\"f1\") # make sure to refit the estimator using \"f1\"\n",
    "\n",
    "# Fit the model on the training set\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best estimator tool to help uncover the best pair combination\n",
    "clf.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What is the best combination of values for the hyperparameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the best average validation score\n",
    "print(\"Best Avg. Validation Score: \", \"%.4f\" % clf.best_score_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.6 Determining the Best Model's Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the decision tree model's accuracy, precision, recall, and F1 score\n",
    "results = pd.DataFrame(columns=['Model', 'F1', 'Recall', 'Precision', 'Accuracy'])\n",
    "\n",
    "def make_results(model_name, model_object):\n",
    "    \"\"\"\n",
    "    Accepts as arguments a model name (your choice - string) and\n",
    "    a fit GridSearchCV model object.\n",
    "\n",
    "    Returns a pandas df with the F1, recall, precision, and accuracy scores\n",
    "    for the model with the best mean F1 score across all validation folds.  \n",
    "    \"\"\"\n",
    "\n",
    "    # Get all the results from the CV and put them in a df.\n",
    "    cv_results = pd.DataFrame(model_object.cv_results_)\n",
    "\n",
    "    # Isolate the row of the df with the max(mean f1 score).\n",
    "    best_estimator_results = cv_results.iloc[cv_results['mean_test_f1'].idxmax(), :]\n",
    "\n",
    "    # Extract accuracy, precision, recall, and f1 score from that row.\n",
    "    f1 = best_estimator_results.mean_test_f1\n",
    "    recall = best_estimator_results.mean_test_recall\n",
    "    precision = best_estimator_results.mean_test_precision\n",
    "    accuracy = best_estimator_results.mean_test_accuracy\n",
    "\n",
    "    # Create a table of results.\n",
    "    table = pd.DataFrame()\n",
    "    table = table.append({'Model': model_name,\n",
    "                          'F1': f1,\n",
    "                          'Recall': recall,\n",
    "                          'Precision': precision,\n",
    "                          'Accuracy': accuracy},\n",
    "                         ignore_index=True)\n",
    "\n",
    "    return table\n",
    "\n",
    "result_table = make_results(\"Tuned Decision Tree\", clf)\n",
    "\n",
    "result_table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Was the additional performance improvement from hyperparameter tuning worth the computational cost? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the plot_tree function to produce a representation of the tree to pinpoint where the splits in the data are occurring. \n",
    "# This will allow review of the \"best\" decision tree.\n",
    "plt.figure(figsize=(20,12))\n",
    "plot_tree(clf.best_estimator_, max_depth=2, fontsize=14, feature_names=X.columns);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.7 Re-evaluate Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncover which features might be most important by building a feature importance graph\n",
    "importances = model.best_estimator_.feature_importances_\n",
    "forest_importances = pd.Series(importances, index=X.columns)\n",
    "\n",
    "# Sort the feature importances in descending order\n",
    "sorted_importances = forest_importances.sort_values(ascending=True)\n",
    "\n",
    "# Create a new figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "\n",
    "# Plot the sorted feature importances with swapped x and y axis\n",
    "sorted_importances.plot.barh(ax=ax)\n",
    "\n",
    "# Set the axis labels and title\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.title('Relative Feature Importance - Random Forest')\n",
    "\n",
    "# Add grid\n",
    "plt.grid(True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Did the feature importance graph re-confirm the previously identified most important features? \n",
    "\n",
    "**Question:** What do you think is the most important metric in this business case?\n",
    "\n",
    "The following are reasons why each metric is important: \n",
    "\n",
    "- Accuracy tends to be the metric that the stakeholders can best understand.\n",
    "\n",
    "- Precision measures what proportion of predicted positives is truly positive. For example, if you wanted to not falsely claim that a customer is satisfied, precision would be a good metric. Assuming a customer is happy when they are really not might lead to customer churn. \n",
    "\n",
    "- Recall measures the percentage of actual positives a model correctly identified (true positive). \n",
    "\n",
    "- F1 balances precision and recall. It is the harmonic mean of precision and recall, or their product divided by their sum."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "provenance": [
    {
     "file_id": "1BgN3Lv1fx-AxyKSqB_2kM3dJ4mFBctv_",
     "timestamp": 1662734078308
    },
    {
     "file_id": "1ZYfhIvPRxnw7ghB_BsNQAMUorLXpAZs_",
     "timestamp": 1658889786811
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
